# ë§¤í¬ë¡œ íƒì§€ ì‹œìŠ¤í…œ
![ì œëª©ì„-ì…ë ¥í•´ì£¼ì„¸ìš”_-001](https://github.com/user-attachments/assets/00cebe44-facd-4a20-9ba3-0f108bc4a5e0)  
- **ìœ ì €ë³„ í‚¤ë³´ë“œ ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•˜ê³  ë°°ì¹˜ì²˜ë¦¬ë¥¼ í†µí•œ ë§¤í¬ë¡œë¥¼ ì‚¬ìš©í•œ ìœ ì € íƒì§€í•˜ëŠ” ì‹œìŠ¤í…œ**  
<br><br>
---  
<br><br>
## ğŸš€ ì‚¬ìš© ê¸°ìˆ   
- **ë¶„ì‚° ì„œë²„ ë°°í¬**: `vagrant`
- **ì½”ë“œ ê´€ë¦¬**: `git`
- **ìš´ì˜ í™˜ê²½ì„ ìœ„í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì¹˜ ë° setup**: `ansible`
- **í‚¤ë³´ë“œ ë¡œê·¸ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: `kafka`
- **ë¶„ì‚° í™˜ê²½ì„ ìœ„í•œ ì• í”Œë¦¬ì¼€ì´ì…˜**: `hadoop`, `hive`, `zookeeper`, `spark`
- **ë°ì´í„° ë² ì´ìŠ¤**: `postgresql`
- **ì›Œí¬í”Œë¡œìš° ë„êµ¬**: `airflow`
- **ì»¨í…Œì´ë„ˆ ë„êµ¬**: `docker-compose`, `docker`
- **ê°„ë‹¨í•œ web**: `flask`
- **ë§¤í¬ë¡œ íƒì§€ ëª¨ë¸**: `lstm`
- **ì™¸ë¶€ ì €ì¥ì†Œ**: `AWS S3`  
<br><br>
---  
<br><br>
## ğŸ“¦ Kafka  
- ìœ ì €ê°€ `play.sh` ì‹¤í–‰ ì‹œ í‚¤ë³´ë“œ ë¡œê·¸ ìˆ˜ì§‘, ì¢…ë£Œ ì‹œ ìˆ˜ì§‘ ì¢…ë£Œí•©ë‹ˆë‹¤.
- **Kafka ë¸Œë¡œì»¤ ìš´ì˜ ì„œë²„**: 3ëŒ€ (ê°€ìƒí™˜ê²½)
- **Kafka ë¸Œë¡œì»¤ ìš´ì˜ ì„œë²„ Crontab**: ë§¤ 23ì‹œì— ë‹¤ìŒ ë‚ ì§œì˜ í† í”½ ìƒì„±í•©ë‹ˆë‹¤.
   ```sh
   0 23 * * * /data/keyboard_sc/make_topic/create_keylog_topic.sh $(date -d tomorrow +"%Y%m%d") >> /data/keyboard_sc/make_topic/create_topic.log 2>&1
   ```
### (2-1) ì¹´í”„ì¹´ í”„ë¡œë“€ì„œ
```python
#!/usr/bin/python3
from evdev import InputDevice, categorize, ecodes, list_devices
import os
import subprocess
import time
from datetime import datetime

from kafka import KafkaProducer

def input_events(broker, topic, username):
    dev = InputDevice('/dev/input/event2')
    producer = KafkaProducer(bootstrap_servers=broker) # í”„ë¡œë“€ì„œ ê°ì²´ ìƒì„±

    try:
        for event in dev.read_loop():
            event_str = str(categorize(event))
            if ("key event at" in event_str) and ("down" in event_str):
                user_key_log = event_str + '\t' + str(username)
                producer.send(topic, user_key_log.encode('utf-8'))
                producer.flush() # ë©”ì‹œì§€ ë°”ë¡œ ì „ì†¡
    finally:
        producer.close() # ê°ì²´ ì¢…ë£Œ

if __name__ == "__main__":
    username=os.getenv('USER')
    today=datetime.now().strftime('%Y%m%d')
    broker_address = ['192.168.219.10:9092', '192.168.219.11:9092', '192.168.219.12:9092'] # kafka ë¸Œë¡œì»¤ ì£¼ì†Œ
    topic_name = f"key_log_{today}" # í† í”½ëª…
    input_events(broker_address, topic_name ,username)
```
- **í•´ë‹¹ í”„ë¡œë“€ì„œë¥¼ í†µí•´ í‚¤ë³´ë“œ ì…ë ¥ ë¡œê·¸ kafka ë¸Œë¡œì»¤ë¡œ ì „ì†¡**  
<br><br>
---  
<br><br>
## ğŸ› ï¸ Airflow  
- **ë§¤ 00ì‹œ ì›Œí¬í”Œë¡œìš° ë°°ì¹˜ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰**
- **Airflow ìš´ì˜ì„œë²„ Crontab : ë§¤ 00ì‹œ ì¹´í”„ì¹´ ì»¨ìŠˆë¨¸ ë³‘ë ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ë¥¼ í†µí•´ ì „ì¼ì í‚¤ë³´ë“œ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°**
  ```sh
  0 0 * * * /usr/bin/python3 /data/keyboard_sc/kafka_consumer/mp_kafka_consumer.py 192.168.219.10:9092,192.168.219.11:9092,192.168.219.12:9092 key_log_$(date +\%Y\%m\%d) /data/keyboard_sc/row_data/keylog/$(date +\%Y\%m\%d)/mp_keylog_$(date +\%Y\%m\%d).txt mp_keylog_$(date +\%Y\%m\%d) >> /data/keyboard_sc/kafka_consumer/kafka_consumer.log 2>&1
  ```
### (3-1) Airflow íŒŒì´í”„ë¼ì¸
<img width="1101" alt="airflow" src="https://github.com/user-attachments/assets/521a07b4-bfed-4ca3-9faf-c5414bd21223">

<br>  

- **ê° task ì„¤ëª…**


1. **`push_xcom`** : airflow íŒŒì´í”„ë¼ì¸ ë³€ìˆ˜ë¥¼ ìœ„í•œ xcom push
2. **`wait_keylog_rowdata`** : ì¹´í”„ì¹´ ì»¨ìŠˆë¨¸ë¥¼ í†µí•¸ ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°í•˜ëŠ” ì„¼ì„œ
3. **`preprocessing_use_spark`** : pysparkë¥¼ í†µí•œ ì›ì‹œ ë°ì´í„° ì „ì²˜ë¦¬
4. **`to_hive`** : ì „ì²˜ë¦¬ ì™„ë£Œëœ ì›ì‹œë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤(hive)ì— ì ì¬
5. **`detection_macro`** : ì •ì¬ëœ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ë§¤í¬ë¡œ íƒì§€ í”„ë¡œê·¸ë¨ ì ìš©í›„ ìµœì¢…ë³¸ postgresqlì— ì ì¬ 
6. **`row_data_zip`** : ì›ì‹œ ë°ì´í„° ì••ì¶• (gzip)
7. **`row_data_zip`** : ì „ì²˜ë¦¬ ë°ì´í„° ì••ì¶• (gzip)
8. **`row_zipfile_to_s3`** : ì••ì¶•ëœ ì›ì‹œë°ì´í„° AWS S3 ì ì¬(ì™¸ë¶€ ì €ì¥ì†Œ)
9. **`row_zipfile_to_hdfs`** : ì••ì¶•ëœ ì›ì‹œë°ì´í„° hdfs ì ì¬(ë‚´ë¶€ ì €ì¥ì†Œ)
10. **`process_zipfile_to_s3`** : ì••ì¶•ëœ ì „ì²˜ë¦¬ ë°ì´í„° AWS S3 ì ì¬(ì™¸ë¶€ ì €ì¥ì†Œ)
11. **`process_zipfile_to_hdfs`** : ì••ì¶•ëœ ì „ì²˜ë¦¬ ë°ì´í„° hdfs ì ì¬(ë‚´ë¶€ ì €ì¥ì†Œ)
12. **`local_zipfile_remove`** : ì••ì¶• ë°ì´í„° ëª¨ë‘ ì ì¬ ì™„ë£Œ í›„ ì‚­ì œ
13. **`check_and_insert_consistency`** : ë°°ì¹˜ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦ì„ ìœ„í•œ ì´ˆê¸°ê°’ postgresqlì— ì‚½ì…
14. **`raw_data_line_count`** : ì›ì‹œë°ì´í„° rawìˆ˜ ì¹´ìš´íŠ¸
15. **`hive_data_line_count`** : ì „ì²˜ë¦¬ë¦¬ë°ì´í„° rawìˆ˜ ì¹´ìš´íŠ¸
16. **`raw_data_awk_count`** : ì›ì‹œë°ì´í„° í•„ë“œìˆ˜ ì¹´ìš´íŠ¸
17. **`decode ~~~`** : ê²€ì¦ì„ ìœ„í•œ ì¹´ìš´íŠ¸ëœ ê°’ ë””ì½”ë”©
18. **`update ~~~`** : êµ¬í•œ ì¹´ìš´íŠ¸ ê°’ìœ¼ë¡œ postgresqlì˜ ê²€ì¦í…Œì´ë¸” update
19. **`rm ~~~`** : ë°ì´í„° ë³´ì¡´ ì •ì±…ì— ë”°ë¼ ë°ì´í„° ì‚­ì œ (local,hive 7ì¼) (hdfs, s3 ë°±ì—… 10ì¼)  
<br><br>
---  
<br><br>
## ğŸ” ë§¤í¬ë¡œ íƒì§€ í”„ë¡œê·¸ë¨  
```python
#!/usr/bin/python3
from postgres.postgres_hook import PostgresConnector
import sys
import psycopg2
from psycopg2 import sql
from pyhive import hive
import pandas as pd
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle

# í‚¤ë¡œê·¸ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
from keylog_functions.keylog_ft import _key_time_ft

def main(batch_date, version):
    conn = hive.Connection(host='192.168.219.20', port=10000, username='sy0218', database='default')
    # ì¿¼ë¦¬ ì‹¤í–‰ (ì‘ì€ ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬)
    query = f"SELECT * FROM keylog_raw WHERE pdate='{batch_date}' and user_id is not null"
    df = pd.read_sql(query, conn)
    conn.close()

    # í•„ìš” ì»¬ëŸ¼ë§Œ ê°€ì ¸ì˜¨í›„ ì •ë ¬
    df = df[['keylog_raw.key_log_time','keylog_raw.key_active','keylog_raw.user_id']].sort_values(by=['keylog_raw.user_id','keylog_raw.key_log_time'], ascending=[True,True])

    group_df = df.groupby('keylog_raw.user_id').agg({
        'keylog_raw.key_log_time': lambda x: list(x.astype(float)),
        'keylog_raw.key_active': lambda x: list(x)
    }).reset_index()
    macro_ai_df = group_df[['keylog_raw.key_active','keylog_raw.user_id']]

    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_path = f"/app/model_materials/macro_model_{version}.keras"
    tokenizer_path = f"/app/model_materials/macro_tokenizer_{version}.pkl"
    model = load_model(model_path)
    with open(tokenizer_path, 'rb') as tk:
        tokenizer = pickle.load(tk)

    # key_active ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜ (max_lengthëŠ” 50, íŒ¨ë”©ì€ post)
    max_length = 50
    def preprocess_key_active(key_active_sequences):
        X = [[tokenizer.get(key, 0) for key in seq] for seq in key_active_sequences]
        return pad_sequences(X, maxlen=max_length, padding='post')

    # key_active í† í° ë³€í™˜( ì „ì²˜ë¦¬ )í›„ ì˜ˆì¸¡
    pred_macro_ai = model.predict(preprocess_key_active(macro_ai_df['keylog_raw.key_active'].tolist()))
    pred_df = pd.DataFrame(pred_macro_ai)
    pred_df.columns = ['pred_result']
    combi_df = pd.concat([group_df, pred_df], axis=1)

    # í‚¤ ì…ë ¥ í…€ ì»¬ëŸ¼ ì¶”ê°€
    combi_df['diff_key_log_time'] = group_df['keylog_raw.key_log_time'].apply(lambda x: [0 if i==0 else round(x[i] - x[i-1], 2) for i in range(len(x))])
    # ë™ì¼í•œ í‚¤ ì…ë ¥í•œ í…€ì¤‘ ìµœëŒ€ ì¹´ìš´íŠ¸ return ( ë§¤í¬ë¡œì‹œ ë™ì¼ ê°„ê²¬ìœ¼ë¡œ í‚¤ë¥¼ ëˆ„ë¥´ë‹ˆê¹Œ ì¹´ìš´íŠ¸ê°€ ë†’ì„ê²ƒ.. )
    combi_df['key_time_pc'] = combi_df['diff_key_log_time'].apply(_key_time_ft)
    result_df = combi_df[['keylog_raw.user_id','pred_result','key_time_pc']]
    result_df['pdate'] = batch_date
    print(result_df)

    # ì»¤ì„œ ìƒì„±
    postgres_conn = PostgresConnector.get_connection('ps_20')
    pg_cr = postgres_conn.cursor()

    # ë©±ë“±ì„± ìœ„í•´ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    pg_cr.execute(f"DELETE FROM macro_pg_result where pdate='{batch_date}'")
    # ë°ì´í„° ì‚½ì…
    for index, row in result_df.iterrows():
        pg_cr.execute(
            sql.SQL("INSERT INTO macro_pg_result (user_id, pred_result, key_time_pc, pdate) VALUES (%s, %s, %s, %s)"),
            (row['keylog_raw.user_id'], row['pred_result'], row['key_time_pc'], row['pdate'])
        )

    # ì»¤ë°‹ ë° ì—°ê²° ì¢…ë£Œ
    postgres_conn.commit()
    pg_cr.close()
    PostgresConnector.dis_connection() # postgres ì¢…ë£Œ

if __name__ == "__main__":
    args = sys.argv[1:]
    # ì¸ìê°€ ë‘ê°œ ì•„ë‹ˆë©´ ì¢…ë£Œ
    if (len(args) != 2):
        print("ì‚¬ìš©ë²• : py <YYYYmmdd> <ê°ì²´ ë²„ì „>")
        sys.exit(1)
    batch_date, version = args
    main(batch_date, version)
```
- **ì‚¬ìš©ì ë³„ë¡œ ë‘ ê°’ì„ ë¦¬í„´**
- **lstm ëª¨ë¸ì„ í†µí•´ ì˜ˆì¸¡ ê²°ê³¼ return ( 1 ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë§¤í¬ë¡œì¼ ê°€ëŠ¥ì„± ë†’ìŒ )**
- **í‚¤ ì…ë ¥í•œ ë™ì¼ ì‹œê°„ í…€ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•œ ê°’ì˜ ìµœëŒ“ê°’ì„ í†µí•´ í‚¤ ë¡œê·¸ í…€ ë¹„ìœ¨ return( ìµœëŒ“ê°’/ì „ì²´ê°’ * 100 )**
- **ì¦‰, ëª¨ë¸ íƒì§€ ê²°ê³¼ì™€ ë™ì¼ í‚¤ ë¡œê·¸ í…€ ë¹„ìœ¨ì´ ë†’ì„ ìˆ˜ë¡ ë§¤í¬ë¡œ ì‚¬ìš©ìì¼ ê²ƒìœ¼ë¡œ íŒë‹¨í• ìˆ˜ ìˆìŠµë‹ˆë‹¤!!**  
<br><br>
---  
<br><br>
## â­ docker-compose  
```sh
version: '3.7'
######### airflow env #########
x-environment: &airflow_environment
    AIRFLOW__CORE__EXECUTOR: "SequentialExecutor"
    AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: "False"
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: "sqlite:////opt/airflow/airflow.db"
    AIRFLOW__CORE__STORE_DAG_CODE: "True"
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
    AIRFLOW__WEBSERVER__RBAC: "False"
    AIRFLOW_CONN_APSERVER_SSH: "ssh://root:nds1101@192.168.219.20:22"
    AIRFLOW_CONN_APSERVER_POSTGRES: "postgresql://sy0218:sy0218@192.168.219.20:5432/sy0218"
######### airflow env #########
services:
  airflow:
    build: /data/keyboard_sc/work_airflow/docker_images/docker_airflow
    image: airflow/airflow_sy0218:2.9.3
    container_name: airflow_container
    ports:
      - "9898:8080"
    volumes:
      - /data/keyboard_sc/work_airflow/dag_airflow/:/opt/airflow/dags/
      - /data/keyboard_sc/work_airflow/pem_dir/:/opt/airflow/pem_dir/
      - /data/keyboard_sc/row_data:/data/keyboard_sc/row_data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - keylog_airflow
    environment: *airflow_environment

  macro_detection_program:
    build: /data/keyboard_sc/macro_detection_program/docker_images/exe_macro_program
    image: exe_macro_dt_pg:1
    container_name: Macro_Detection_Program
    restart: "no"

  make_macro_detection_program_model:
    build: /data/keyboard_sc/macro_detection_program/docker_images/mk_macro_program
    image: macro_dt_pg:0.0.1
    container_name: Make_Macro_Detection_Program_Model
    restart: "no"

  flask_macro_application:
    build: /data/keyboard_sc/flask_macro
    image: flask_macro_app:0.0.1
    container_name: macro_app_container
    ports:
      - "7777:5000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - keylog_airflow

networks:
  keylog_airflow:
    name: keylog_airflow
```
- **ê° ì• í”Œë¦¬ì¼€ì´ì…˜ ë³„ ê²©ë¦¬ë¥¼ ìœ„í•´ 4ê°€ì§€ ì»¨í…Œì´ë„ˆë¥¼ êµ¬í˜„ í•˜ì˜€ìŠµë‹ˆë‹¤.**  
<br><br>
---  
<br><br>

## ğŸŒ UI ì„¤ëª…  

### (6-1) ë©”ì¸ í˜ì´ì§€
<img width="1055" alt="ui_main" src="https://github.com/user-attachments/assets/b3a80079-2003-431f-84c6-f0ea7fee8829">

- **íŠ¹ì • ë‚ ì§œì˜ ë§¤í¬ë¡œ íƒì§€ ê²°ê³¼ì™€ ë°ì´í„° ê²€ì¦ê´€ë ¨ ì •ë³´ë¥¼ í™•ì¸í• ìˆ˜ ìˆìŠµë‹ˆë‹¤.**  

### (6-2) ëª¨ë¸ ìƒì„± ë©”ë‰´
<img width="412" alt="model_m_page" src="https://github.com/user-attachments/assets/a4399072-a2ac-4733-a385-264bd3a2d709">

- **ë‹´ë‹¹ìê°€ ì§ì ‘ ëª¨ë¸ ë²„ì „ì„ ì…ë ¥í›„ ìƒì„±í• ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

### (6-3) ì—ì–´í”Œë¡œìš° UI ë©”ë‰´
<img width="1259" alt="airflow_ui" src="https://github.com/user-attachments/assets/f0ee04cf-5676-4992-8365-c28ddaafd726">

- **Airflow UIì— ì ‘ì†í• ìˆ˜ ìˆëŠ” ë©”ë‰´ ì…ë‹ˆë‹¤.**  

<br><br>
---  
<br><br>

## ğŸ–¥ï¸ ìš´ì˜ì„œë²„ êµ¬ì¶•ì„ ìœ„í•œ ansible ì…ë‹ˆë‹¤!!  
![ì œëª©ì„-ì…ë ¥í•´ì£¼ì„¸ìš”_-002](https://github.com/user-attachments/assets/9bdd02de-d92a-43c5-bd2c-156e596face9)

- **[kafka, ì• í”Œë¦¬ì¼€ì´ì…˜ ì„œë²„] ìš´ì˜ì²´ì œ : Ubuntu22.04**
- **https://github.com/sy0218/os_auto_ansible : (ubuntu 22.04) ìš´ì˜ì²´ì œ ìë™ setup ansible**
- **https://github.com/sy0218/ssh_keygen_auto_scripty : ssh-keygen ìë™ ìŠ¤í¬ë¦½íŠ¸**
- **https://github.com/sy0218/zookeeper_3.7.2_ansible : ì£¼í‚¤í¼ ìë™ ì„¤ì¹˜ ë° ë™ì  setup anible**
- **https://github.com/sy0218/kafka_3.6.2_auto_ansible : kafka ìë™ ì„¤ì¹˜ ë° ë™ì  setup anible**
- **https://github.com/sy0218/postgresql_12_ansible : postgresql ìë™ ì„¤ì¹˜ ë° ë™ì  setup anible**
- **https://github.com/sy0218/hadoop_3.3.5_ansible : í•˜ë‘¡ ìë™ ì„¤ì¹˜ ë° ë™ì  setup anible**
- **https://github.com/sy0218/hive_3.1.3_ansible : hive ìë™ ì„¤ì¹˜ ë° ë™ì  setup anible**
- **https://github.com/sy0218/spark_3.4.3_ansible : ìŠ¤íŒŒí¬ ìë™ ì„¤ì¹˜ ë° ë™ì  setup anible**
- **https://github.com/sy0218/check : ìš´ì˜ì— ìœ ìš©í•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ëª¨ì•„ë†“ì€ ë ˆí¬ì§€í† ë¦¬ ì…ë‹ˆë‹¤!!**  


<br><br>
---  
<br><br>

# ì´ìƒìœ¼ë¡œ ë§¤í¬ë¡œ íƒì§€ ì‹œìŠ¤í…œ í”„ë¡œì íŠ¸ì˜€ìŠµë‹ˆë‹¤ ê°ì‚¬í•©ë‹ˆë‹¤!!ğŸ™ğŸ™  

